---
title: "Interpretability and Explainability in Machine Learning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load the concrete dataset
library(readxl)
concrete <- as.data.frame(read_excel("Concrete_Data.xls"))
DescVars <- names(concrete)
names(concrete) <- c(
  "Cement", "Slag", "FlyAsh", "Water", "Superplast",
  "CoarseAggr", "FineAggr", "Age", "Strength"
)
```

**Data processing: Creating training and test sets**\
Create a training sample choosing 700 data at random. The non-chosen data will be the test set.

```{r}
# Set a seed for reproducibility
set.seed(123456)

# Create indices for the training set
train_indices <- sort(sample(1:nrow(concrete), size = 700, replace = FALSE))

# Split the data into training and test sets
train_set <- concrete[train_indices, ]
test_set <- concrete[-train_indices, ]

# Check the dimensions of the splits
cat("Training set dimensions: ", dim(train_set), "\n")
cat("Test set dimensions: ", dim(test_set), "\n")
```

**1. Fit a Random Forest**

a.  Compute the *Variable Importance* by the reduction of the **impurity** at the splits defined by each variable.

```{r}
library(ranger)

model_rf_imp <- ranger(
  Strength ~ .,
  data = train_set,
  importance = "impurity"
)
print(model_rf_imp)
```

b.  Compute the *Variable Importance* by out-of-bag random permutations.

```{r}
model_rf_perm <- ranger(
  Strength ~ .,
  data = train_set,
  importance = "permutation"
)
print(model_rf_perm)
```

c.  Do a graphical representation of both *Variable Importance* measures.

```{r}
library(vip)
library(gridExtra)

rf_imp_vip <- vip(model_rf_imp, num_features = 8)
rf_perm_vip <- vip(model_rf_perm, num_features = 8)
grid.arrange(rf_imp_vip, rf_perm_vip,
  ncol = 2,
  top = "Left: Reduction in impurity. Right: Out-of-bag permutations"
)
```

d.  Compute the *Variable Importance* of each variable by Shapley Values.

```{r}
library(DALEX)

rf_shapley <- vip(
  model_rf_imp,
  method = "shap",
  pred_wrapper = yhat,
  num_features = 8,
  train = train_set,
  newdata = test_set[, -which(names(test_set) == "Strength")]
)

grid.arrange(rf_imp_vip, rf_perm_vip, rf_shapley,
  ncol = 2, nrow = 2,
  top = "Top left: Impurity. Top right: OOB permutations. Bottom: Shapley values"
)
```

**2. Fit a linear model and a gam model**

a.  Summarize, numerically and graphically, the fitted models.

```{r}
# Linear model fit
lm_concrete <- lm(Strength ~ ., data = train_set)

# Numerical summary of the fitted model
(summ_lm_concrete <- summary(lm_concrete))

# Graphical summary for the linear model
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
plot(lm_concrete)

```

```{r}
library(mgcv)

# Fit a Generalized Additive Model (GAM)
gam_concrete <- gam(Strength ~ s(Cement) + s(Slag) + s(FlyAsh) + 
                      s(Water) + s(Superplast) + s(CoarseAggr) + 
                      s(FineAggr) + s(Age), 
                    data = train_set)

# Numerical summary of the fitted model
(summ_gam_concrete <- summary(gam_concrete))

# Graphical summary for the GAM model
par(mfrow = c(3, 3))
plot(gam_concrete, residuals = TRUE, rug = TRUE, pages = 1)
```

b.  Compute the *Variable Importance* by Shapley values in the linear and gam fitted models. Compare your results with what you have learned before.

```{r}
# Compute Shapley values for the linear model
lm_concrete_shapley <- vip(
  lm_concrete, 
  method = "shap",
  pred_wrapper = predict.lm,  
  train = train_set,          
  newdata = test_set,        
  num_features = ncol(train_set) - 1,  
  exact = TRUE                
)

# Plot Shapley values for the linear model
plot(lm_concrete_shapley)

```

```{r}
# Compute Shapley values for the GAM
gam_concrete_shapley <- vip(
  gam_concrete, 
  method = "shap",
  pred_wrapper = predict.gam,  
  train = train_set,          
  newdata = test_set,          
  num_features = ncol(train_set) - 1,  
  exact = TRUE               
)

# Plot Shapley values for the GAM
plot(gam_concrete_shapley)

```

```{r}
# Combine all plots
grid.arrange(
  rf_imp_vip, 
  rf_shapley, 
  lm_concrete_shapley, 
  gam_concrete_shapley, 
  ncol = 2, nrow = 2,
  top = "1,1: RF Impurity. 1,2: Shapley RF. 2,1: Shapley LM. 2,2: Shapley GAM"
)

```

**3.Relevance by Ghost Variables**

Compute the relevance by ghost variables in the three fitted models.

```{r}
library(grid)
library(ggplot2)
source("relev.ghost.var.R")  
```

```{r}
# GAM model
Rel_Gh_Var_gam <- relev.ghost.var(
  model = gam_concrete,                   
  newdata = test_set[, -which(names(test_set) == "Strength")], 
  y.ts = test_set$Strength,               
  func.model.ghost.var = lm
)

par(mar = c(4, 4, 2, 1))
plot.relev.ghost.var(
  Rel_Gh_Var_gam,
  n1 = 500,    
  ncols.plot = 3 
)

# Combine ghost variable relevance with Shapley values
aux <- cbind(
  Rel_Gh_Var_gam$relev.ghost, 
  gam_concrete_shapley$data$Importance
)

# Scatter plot
plot(aux[, 1], aux[, 2], 
     col = 0, 
     xlab = "Relevance by Ghost Variables", 
     ylab = "Shapley Variable Importance",
     main = "GAM model")

# Add text labels for each variable
text(aux[, 1], aux[, 2], labels = row.names(aux))

```

```{r}
Rel_Gh_Var_lm <- relev.ghost.var(
  model = lm_concrete,
  newdata = test_set[, -which(names(test_set) == "Strength")], 
  y.ts = test_set$Strength,                                   
  func.model.ghost.var = lm                              
)

par(mar = c(4, 4, 2, 1))
plot.relev.ghost.var(
  Rel_Gh_Var_lm,
  n1 = 500,    
  ncols.plot = 3 
)

# Combine ghost variable relevance with Shapley values
aux <- cbind(
  Rel_Gh_Var_lm$relev.ghost, 
  lm_concrete_shapley$data$Importance
)

# Scatter plot
plot(aux[, 1], aux[, 2], 
     col = 0, 
     xlab = "Relevance by Ghost Variables", 
     ylab = "Shapley Variable Importance",
     main = "Linear model")

# Add text labels for each variable
text(aux[, 1], aux[, 2], labels = row.names(aux))

```

```{r}
model_rf_imp <- ranger(
  x          = train_set[, setdiff(names(train_set), "Strength")],
  y          = train_set$Strength,
  importance = "impurity"
)
```

```{r}
# I was having problems with the "relev.ghost.var" function when using the "ranger" package to fit the random forest. I have changed to the "randomForest" library and then it works. The model should be the same as the one fitted before since we set a seed and specify the same number of trees.

library(randomForest)

model_rf <- randomForest(
  Strength ~ .,   
  data = train_set,
  importance = TRUE, 
  ntree = 500     
)
print(model_rf)
```

```{r}
Rel_Gh_Var_rf <- relev.ghost.var(
  model = model_rf,
  newdata = test_set, 
  y.ts = test_set$Strength,                                   
  func.model.ghost.var = lm
)

par(mar = c(4, 4, 2, 1))
plot.relev.ghost.var(
  Rel_Gh_Var_rf,
  n1 = 500,    
  ncols.plot = 3 
)

# Combine ghost variable relevance with Shapley values
aux <- cbind(
  Rel_Gh_Var_rf$relev.ghost, 
  rf_shapley$data$Importance
)

# Scatter plot
plot(aux[, 1], aux[, 2], 
     col = 0, 
     xlab = "Relevance by Ghost Variables", 
     ylab = "Shapley Variable Importance",
     main = "Random forest model")

# Add text labels for each variable
text(aux[, 1], aux[, 2], labels = row.names(aux))
```

**4. Global Importance Measures and Plots using the library DALEX**

a. Compute Variable Importance by Random Permutations
b. Do the Partial Dependence Plot for each explanatory variable.
c. Do the Local (or Conditional) Dependence Plot for each explanatory variable.

```{r, message=FALSE, warning=FALSE}

# Create an explainer for the existing model `model_rf_imp` from previous section
explainer <- explain(
  model = model_rf_imp,
  data = train_set[, -ncol(train_set)], # Explanatory variables
  y = train_set$Strength,              # Target variable
  label = "Random Forest - Impurity"
)

# (a) Compute Variable Importance by Random Permutations
importance <- model_parts(explainer)
print(importance)
plot(importance)

# (b) Partial Dependence Plots for each explanatory variable
pdp <- model_profile(explainer)
print(pdp)
plot(pdp)

# (c) Local (Conditional) Dependence Plots
# Example: First observation in the test set
local_profile <- predict_profile(explainer, new_observation = test_set[1, -ncol(test_set)])
print(local_profile)
plot(local_profile)
```


**5. Local explainers with library DALEX**

 Choose two instances in the the test set, the prediction for which we want to explain:
 • The data with the lowest value in Strength.
 • The data with the largest value in Strength.
 For these two instances, do the following tasks for the fitted random forest.
 a. Explain the predictions using SHAP.
 b. Explain the predictions using Break-down plots.
 c. Explain the predictions using LIME.
 d. Do the Individual conditional expectation (ICE) plot, or ceteris paribus plot
 e. Plot in one graphic the Individual conditional expectation (ICE) plot for variable Age for eachcase in the test sample. Add the global Partial Depedence Plot
 
Identifying the instances 
 
```{r}

lowest_strength <- test_set[which.min(test_set$Strength), ]
highest_strength <- test_set[which.max(test_set$Strength), ]

lowest_strength
highest_strength


```
a. Explain the predictions using SHAP

```{r}

# SHAP lowest Strength instance
bd_rf_low <- predict_parts(explainer = explainer,
                           new_observation = lowest_strength,
                           type = "shap")
plot(bd_rf_low)

# SHAP highest Strength instance
bd_rf_high <- predict_parts(explainer = explainer,
                            new_observation = highest_strength,
                            type = "shap")
plot(bd_rf_high)


```

b. Explain the predictions using Break-down plots.

```{r}

# Break-down lowest Strength instance
bd_rf_low <- predict_parts(explainer = explainer,
                           new_observation = lowest_strength,
                           type = "break_down")
plot(bd_rf_low, max_features = 11)

# Break-down highest Strength instance
bd_rf_high <- predict_parts(explainer = explainer,
                            new_observation = highest_strength,
                            type = "break_down")
plot(bd_rf_high, max_features = 11)


```

c. Explain the predictions using LIME

```{r}
# LIME lowest Strength instance
lime_rf_low <- predict_surrogate(explainer = explainer,
                                 new_observation = lowest_strength,
                                 type = "localModel")
plot(lime_rf_low)

# LIME highest Strength instance
lime_rf_high <- predict_surrogate(explainer = explainer,
                                  new_observation = highest_strength,
                                  type = "localModel")
plot(lime_rf_high)

```
d. Do the Individual conditional expectation (ICE) plot, or ceteris paribus plot

```{r}

# ICE lowest Strength instance
cp_rf_low <- predict_profile(explainer = explainer,
                             new_observation = lowest_strength)
plot(cp_rf_low, facet_ncol = 2)

# ICE highest Strength instance
cp_rf_high <- predict_profile(explainer = explainer,
                              new_observation = highest_strength)
plot(cp_rf_high, facet_ncol = 2)


```



e. Plot in one graphic the Individual conditional expectation (ICE) plot for variable Age for each case in the test sample. Add the global Partial Depedence Plot



```{r}

# ICE for age
ice_profiles <- predict_profile(
  variables = "Age",
  explainer = explainer,
  new_observation = test_set
)
ice_profiles

#no detecta be la variable age i no es pot fer el grafic, com veieu en aquest grafic apareixen totes les variables
plot(ice_profiles, facet_ncol = 2)


# Plot ICE profiles
#plot(ice_profiles, geom = "profiles") +
#  ggtitle("Individual Conditional Expectation (ICE) Plot for Age")

#Partial Dependence Plot

pdp_rf <- model_profile(explainer = explainer,
  variables = "Age",
  N = 100,
  type = "partial"
)


plot(pdp_rf, geom = "profiles") +  
  ggtitle("Ceteris-paribus and partial-dependence profiles for temp")

```

